---
layout: default
title: Mini Demo
permalink: /mini-demo/
---

# Mini Demo

Since it costs a lot to upload our program to a server, we made a mini demo that describes our idea. When users upload an audio file, the web analyzes its characteristics, generates a descriptive text caption, and creates a new music track based on the caption.

analyze_audio(file_path)

*  Input:
	  *  The file path of the uploaded audio file.
*  How It Works:
	  * Uses the librosa library to analyze the audio file.
	  * Extracts the following features: Tempo (BPM), Spectral Centroid, RMS Energy, Zero Crossing Rate
*  Output:
	  *  Returns a dictionary containing these features\

generate_music_caption(features)

*  Input:
	  *  A dictionary of audio features generated by analyze_audio.
*  How It Works:
	  *  Constructs a prompt using the audio features.
      *  Sends the prompt to OpenAI’s GPT-3.5 model via the OpenAI API.
      *  The AI processes the prompt and generates a text caption describing:
      *  The mood (e.g., happy, melancholic).
      *  The style (e.g., classical, jazz).
      *  Potential use cases (e.g., background music for a video).
*  Output:
      *  A textual caption


generate_music_with_musicgen(caption)

*  Input:
	  *  The caption generated by generate_music_caption.
*  How It Works:
	*  Sends the caption to Meta’s MusicGen API through the replicate library.
    *  MusicGen processes the caption and generates a new audio track in MP3 format.
    *  Saves the generated audio to the static directory for easy access via the web interface.
*  Output:
	* Returns the path to the newly generated MP3 file.

## Workflow
1. **User Uploads**: An audio file.
2. **Feature Extraction**: `analyze_audio` extracts features.
3. **Caption Generation**: `generate_music_caption` creates a descriptive text.
4. **Music Generation**: `generate_music_with_musicgen` generates a new music track based on the caption.
5. **Results**: Features, captions, and generated music are displayed on the web page.